from langchain_core.messages import HumanMessage
from src.models.ollama_model import OllamaModel
from src.tools.rag_tool import rag_tool
from src.utils.logger import get_logger
from langgraph.prebuilt import create_react_agent
from src.config import MODEL_LLAMA_ANALYZER

logger = get_logger(__name__)

async def analyst_node(state, config):
    """
    1ï¸âƒ£ ğŸ¦™ Llama 3.1 (Analist & RAG UzmanÄ±)
    """
    logger.info("Llama Analist (Analyst) Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor")
    
    messages = state["messages"]
    
    # Llama 3.1 Modelini YÃ¼kle
    model_client = OllamaModel(model_name=MODEL_LLAMA_ANALYZER, temperature=0.1)
    model = model_client.llm
    
    tools = [rag_tool]
    
    system_prompt = """Sen Llama 3.1, bu sistemin 'Analist ve RAG UzmanÄ±'sÄ±n.
    GÃ¶revin:
    1. KullanÄ±cÄ± sorgusunu analiz et.
    2. RAG tool'unu kullanarak dokÃ¼manlardan ilgili bilgileri Ã§ek.
    3. Ã‡ektiÄŸin bilgileri (Context) birleÅŸtir ve yorumla.
    4. EÄŸer matematiksel hesaplama veya kod gerekiyorsa bunu belirt.
    5. Sonraki aÅŸama iÃ§in Gemini'ye (Master) hitaben net, yapÄ±landÄ±rÄ±lmÄ±ÅŸ bir rapor hazÄ±rla.
    
    Ã‡Ä±ktÄ±n ÅŸunlarÄ± iÃ§ermelidir:
    - **BULGULAR:** DokÃ¼manlardan elde edilen veriler.
    - **ANALÄ°Z:** Bu verilerin yorumu.
    - **GEREKSÄ°NÄ°MLER:** (Varsa) Hesaplama veya ek araÅŸtÄ±rma ihtiyacÄ±.
    - **GEMINI Ä°Ã‡Ä°N PROMPT:** Gemini'nin son cevabÄ± Ã¼retmesi iÃ§in talimat.
    """
    
    # LangGraph create_react_agent kullanÄ±mÄ±
    agent = create_react_agent(model, tools, state_modifier=system_prompt)
    
    # "messages" key'ini kullanarak invoke ediyoruz
    # create_react_agent, input olarak {"messages": ...} bekler
    response = await agent.ainvoke({"messages": messages})
    
    # Son mesajÄ± al (AIMessage)
    final_message = response["messages"][-1]
    
    # HumanMessage olarak sarmalayÄ±p dÃ¶ndÃ¼rÃ¼yoruz ki Graph akÄ±ÅŸÄ±nda 'analyst' olarak gÃ¶rÃ¼nsÃ¼n
    return {"messages": [HumanMessage(content=final_message.content, name="analyst")]}
