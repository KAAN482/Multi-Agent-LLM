from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.tools import tool
from src.tools.rag_tool import rag_tool
from src.utils.logger import get_logger
from langchain.agents import AgentExecutor, create_tool_calling_agent
from src.models.ollama_model import OllamaModel
from src.config import MODEL_LLAMA_ANALYZER

logger = get_logger(__name__)

async def analyst_node(state, config):
    """
    1ï¸âƒ£ ğŸ¦™ Llama 3.1 (Analist & RAG UzmanÄ±)
    
    GÃ¶revleri:
    - KullanÄ±cÄ± sorgu analizi
    - RAG context birleÅŸtirme
    - DokÃ¼man + tablo yorumlama
    - Ä°lk reasoning
    - Geminiâ€™ye gidecek promptâ€™u hazÄ±rlama
    """
    logger.info("Llama Analist (Analyst) Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor")
    
    messages = state["messages"]
    last_message = messages[-1]
    
    # Llama 3.1 Modelini YÃ¼kle
    model_client = OllamaModel(model_name=MODEL_LLAMA_ANALYZER, temperature=0.1) # Analiz iÃ§in dÃ¼ÅŸÃ¼k sÄ±caklÄ±k
    model = model_client.llm
    
    tools = [rag_tool]
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """Sen Llama 3.1, bu sistemin 'Analist ve RAG UzmanÄ±'sÄ±n.
        GÃ¶revin:
        1. KullanÄ±cÄ± sorgusunu analiz et.
        2. RAG tool'unu kullanarak dokÃ¼manlardan ilgili bilgileri Ã§ek.
        3. Ã‡ektiÄŸin bilgileri (Context) birleÅŸtir ve yorumla.
        4. EÄŸer matematiksel hesaplama veya kod gerekiyorsa bunu belirt.
        5. Sonraki aÅŸama iÃ§in Gemini'ye (Master) hitaben net, yapÄ±landÄ±rÄ±lmÄ±ÅŸ bir rapor hazÄ±rla.
        
        Ã‡Ä±ktÄ±n ÅŸunlarÄ± iÃ§ermelidir:
        - **BULGULAR:** DokÃ¼manlardan elde edilen veriler.
        - **ANALÄ°Z:** Bu verilerin yorumu.
        - **GEREKSÄ°NÄ°MLER:** (Varsa) Hesaplama veya ek araÅŸtÄ±rma ihtiyacÄ±.
        - **GEMINI Ä°Ã‡Ä°N PROMPT:** Gemini'nin son cevabÄ± Ã¼retmesi iÃ§in talimat.
        """),
        ("placeholder", "{chat_history}"),
        ("human", "{input}"),
        ("placeholder", "{agent_scratchpad}"),
    ])
    
    agent = create_tool_calling_agent(model, tools, prompt)
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
    
    # Asenkron Ã§aÄŸrÄ±
    response = await agent_executor.ainvoke({"input": last_message.content, "chat_history": messages[:-1]})
    
    return {"messages": [HumanMessage(content=response["output"], name="analyst")]}
