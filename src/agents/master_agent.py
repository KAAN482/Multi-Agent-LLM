from langchain_core.messages import HumanMessage
from src.models.gemini_model import GeminiModel
from src.config import MODEL_GEMINI_MASTER
from src.utils.logger import get_logger
from src.tools.web_search import web_search_tool
from langgraph.prebuilt import create_react_agent

logger = get_logger(__name__)

async def master_agent_node(state, config):
    """
    2ï¸âƒ£ ğŸŒ Gemini 2.5 Flash (Master & Web Agent)
    """
    logger.info("Gemini Master Agent Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor")
    
    messages = state["messages"]
    
    # Mesaj geÃ§miÅŸini analiz et (Manuel context hazÄ±rlÄ±ÄŸÄ±)
    analyst_msg = next((m for m in reversed(messages) if m.name == "analyst"), None)
    logic_msg = next((m for m in reversed(messages) if m.name == "logic_expert"), None)
    
    # GiriÅŸ (Input) hazÄ±rlÄ±ÄŸÄ±
    original_user_msg = messages[0]
    user_input = original_user_msg.content
    
    context_str = ""
    if analyst_msg:
        context_str += f"\n[ANALÄ°ST RAPORU]:\n{analyst_msg.content}\n"
    if logic_msg:
        context_str += f"\n[MANTIK UZMANI SONUCU]:\n{logic_msg.content}\n"
        
    final_query = f"KullanÄ±cÄ± Sorusu: {user_input}\n\nEldeki BaÄŸlam:{context_str}\n\nGÃ¶revin: Bu bilgileri kullanarak nihai cevabÄ± Ã¼ret."

    # Gemini Modeli
    model_client = GeminiModel(model_name=MODEL_GEMINI_MASTER, temperature=0.7)
    model = model_client.llm
    
    tools = [web_search_tool]
    
    system_prompt = """Sen Gemini 2.5 Flash, bu sistemin 'Master Agent'Ä±sÄ±n. En Ã¼st dÃ¼zey karar vericisin.
    
    GÃ¶revlerin:
    1. Llama (Analist) ve DeepSeek (MantÄ±k) ajanlarÄ±ndan gelen raporlarÄ± deÄŸerlendir.
    2. EÄŸer raporda eksik bilgi varsa veya gÃ¼ncel bilgi gerekiyorsa 'web_search' aracÄ±nÄ± kullan.
    3. Ajan Ã§Ä±ktÄ±larÄ± arasÄ±nda Ã§eliÅŸki varsa tespit et ve doÄŸrusunu bul.
    4. Sonucu akademik, yapÄ±landÄ±rÄ±lmÄ±ÅŸ ve detaylÄ± bir formatta kullanÄ±cÄ±ya sun.
    """
    
    agent = create_react_agent(model, tools, state_modifier=system_prompt)
    
    # Master iÃ§in yeni bir mesaj dizisi oluÅŸturuyoruz.
    # Sadece final_query'i gÃ¶nderiyoruz Ã§Ã¼nkÃ¼ context zaten iÃ§inde.
    # (Chat history'yi olduÄŸu gibi verirsek model kafasÄ± karÄ±ÅŸabilir, summary yeterli)
    master_messages = [HumanMessage(content=final_query)]
    
    response = await agent.ainvoke({"messages": master_messages})
    
    final_message = response["messages"][-1]
    
    return {"messages": [HumanMessage(content=final_message.content, name="master")]}
